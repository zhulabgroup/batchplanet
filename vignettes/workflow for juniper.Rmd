---
title: "workflow for juniper"
author: "Yiluan Song"
date: "2023-10-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(urbanplanet)
```

## Read street tree inventory

```{r}
df_tree <- misc_read_juniper_coord(version = NULL)
```

Area too big, so divide them into multiple sites (counties).
```{r}
sp_county <- read_county(country = "US")
# sp_county_sub <- read_county_sub(country = "US", state = "texas") # county subdivision not used
df_tree_county <- util_coord_in_area(df_tree, sp_county, field = "NAME")
```

## Download PS data
Set some parameters. Replace API key with your own here.
```{r}
setting <- set_ps(api_key = "REMOVED")
```

Order. Order IDs saved as rds.
```{r}
order_ps_batch(dir = "alldata/juniper_ashei/PSdata/", df_plant = df_tree_county, v_site = NULL, setting = setting)
```

Here, check your Planet account to make sure orders are completed. Make sure all orders are "success" without any order "failed." Once all orders are completed, please do not order the same images repeatedly.

Download. Use Great Lakes R session with 12 cores for this step.
```{r}
down_ps_batch(dir = "alldata/juniper_ashei/PSdata/", v_site = "all", setting = setting)
```
Here, tar and compress folders for cities downloaded by submitting Slurm jobs and then move tarred compressed files to Data Den.

## Retrieve time series

Extract time series at tree locations. Use Great Lakes R session with 36 cores for this step.
```{r}
proc_ps_ts(dir = "alldata/juniper_ashei/PSdata/", df_plant = df_tree, v_site = NULL)
```

Time series preprocessing, including removing low quality data and calculating EVI.
```{r}
proc_evi_ts(dir = "alldata/juniper_ashei/PSdata/", v_site = NULL)
```

Read in data and visualize.
```{r}
df_evi <- read_evi(dir = "alldata/PSdata/", v_site = NULL)
df_evi
```

