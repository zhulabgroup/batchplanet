---
title: "Vignette"
author: "Yiluan Song, Ken Reid"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This workflow demonstrates the features of the _batchplanet_ package. It provides a step-by-step guide to downloading PlanetScope imagery, processing time series data, and calculating phenological metrics. The package is designed to work with the PlanetScope API and is particularly useful for researchers and practitioners in environmental studies and remote sensing.

Who should use this package?
- If you prioritize reproducibility and control over data.
- If you prefer using R to python.
- If you want to download PlanetScope imagery over multiple sites and over a long time (optional but recommended).
- If you use high-performance computing (optional).
- If you want to retrieve and process time series from PlanetScope data (optional).

```{r}
library(batchplanet)
```

## 1 Read coordinate data
In order to download PlanetScope imagery, you need to provide coordinates of points of interest. For this example, we will use a sample data file retrieved from the [National Ecological Observatory Network (NEON)](https://www.neonscience.org/) [plant phenology observations data product](https://data.neonscience.org/data-products/DP1.10055.001).
```{r}
# Read the data file wilth coordinates
df_coordinates <- read_csv(system.file("extdata", "example_neon_coordinates.csv", package = "batchplanet"), show_col_types = FALSE)
head(df_coordinates)
```
You should create your own data frame with coordinates. Make sure your data frame for coordinates have the columns: `id` (unique id for each point of interest), `lon` for longitude, and `lat` for latitude. You may have a character column `site` if you have clustered coordinates at dispersed sites. You may also a character column `group` you can use to group coordinates later on.

What do we mean by "clustered coordinates at dispersed sites?" Use the function `visualize_coordinates()` to visualize the coordinates. Our sample coordinates are dispersed across 47 NEON sites. Within each NEON site, there are above 200 coordinates in a square, representing individual trees tagged for phenology observations.
```{r}
visualize_coordinates(df_coordinates) # Multiple sites across continental US
visualize_coordinates(df_coordinates %>% filter(site == "SJER")) # Zoom in to one site SJER
```
Why is it important to label the coordinates with sites? If we try to download PlanetScope imagery that cover all coordinates, we will end up with a tremendous amount of imagery that cover most of continental US. Instead, we will use the site labels to create bounding boxes that cover each site. This way, we can download imagery that only cover the sites of interest. If your coordinates are already clustered in a small area, you don't have to supply the site labels. The package will process all coordinates as a single site.

## 2 Download PS data

You will need an active Planet account and an API key to access the PlanetScope API. You can sign up for an account on the [Planet website](https://www.planet.com/get-started/). Once you have an account, you can copy your API key from your account settings.

### 2.1 Set downloading parameters

You will specify several key parameters for downloading PlanetScope imagery using the `set_planetscope_parameters()` function. These parameters include the API key, item name, asset type, product bundle, cloud cover limit, and whether to use harmonized data. 

Within this step, you set your API key using the `set_api_key()` function. This function will save your API key in a hidden environment file in your working directory, so you don't have to enter it every time you run the code. If you would like to change your API key, you can use the same function with the argument `change_key = T`.

>In the Data API, an __item__ is an entry in our catalog, and generally represents a single logical observation (or scene) captured by a satellite. Items have __assets__, which are the downloadable products derived from the item's source data. [PlanetScope documentation](https://docs.planet.com/develop/apis/data/items/#assets)

> __Product bundles__ comprise of a group of assets for an item. [PlanetScope documentation](https://docs.planet.com/develop/apis/orders/product_bundles/)

Our default item is `PSScene`, 8-band PlanetScope imagery. The default asset is `ortho_analytic_4b_sr`, which is a PlanetScope atmospherically corrected surface reflectance product with four bands (blue, green, red, near-infrared). The default product bundle is `analytic_sr_udm2`, which includes the surface reflectance data and the [Usable data mask 2 (UDM2)](https://docs.planet.com/data/imagery/udm/). You can change these parameters to suit your needs. See a full list of PlanetScope items and assets in the [PlanetScope data catalog](https://docs.planet.com/data/) and see information and product bundles [here](https://docs.planet.com/develop/apis/orders/product_bundles/). 

PlanetScope API allows filtering imagery by several criteria. Here, we allow users to filter imagery below a certain cloud coverage (the ratio of the area covered by clouds to total area). The `cloud_lim` parameter is set to 1 by default, which means we will download all images regardless of cloud coverage. You can set this parameter to a lower value (e.g., 0.5) to filter out images with more than 50% cloud coverage.

PlanetScope API allow users to apply a tool named "harmonize" that applies scene-level normalization and harmonization, such that all PlanetScape data were consistent and approximately comparable to data from Sentinel 2. This tool is useful when integrating or comparing images from different times and locations. Refer to [PlanetScope technical documentation](https://assets.planet.com/docs/scene_level_normalization_of_planet_dove_imagery.pdf) for more details. The `harmonized` parameter is set to `TRUE` by default, which means we will download harmonized data. You can set this parameter to `FALSE` to download non-harmonized data.

```{r}
setting <- set_planetscope_parameters(
  api_key = set_api_key(),
  item_name = "PSScene",
  asset = "ortho_analytic_4b_sr",
  product_bundle = "analytic_sr_udm2",
  cloud_lim = 1,
  harmonized = T
)
```

Use the `set_data_directory()` function to specify a directory to store the downloaded data and later store processed data. It is recommended to use symbolic link to a directory on your HPC system. This way, you can easily access the data from your local machine without being limited by the storage space on your local machine. Please still closely monitor the storage space on your local machine or HPC system as the data can be large. For example, all images for New York City since 2017 can be over 1TB.
```{r}
dir_data <- set_data_directory()
```

### 2.2 Order

Use the `order_planetscope_imagery_batch()` function to order PlanetScope imagery over multiple sites and years. This function first searches for all the images that meets the criteria you specified, and then orders the images.

You can specify the sites (must exist in your `site` column of the coordinates data frame) and years (after 2014) you want to order images for. Here, we order images from two sites in one year as an example. If you do not specify `v_site` and `v_year`, the function will order images for all sites and years in the coordinates data frame.

The function will create a folder `raw/` in the data directory you specified, and multiple subfolders for different sites you specified. In each subfolder, the function will save the order IDs in rds files for later use.

If you would like to customize your batch downloading process, you may use the functions `search_planetscope_imagery()` and `order_planetscope_imagery()` to search and order images for a single site and time period. Be careful not to order too many images at once, as the Planet API has an internal limit. The `order_planetscope_imagery_batch()` function is designed to handle multiple sites and years in a more streamlined manner.

```{r, eval = F}
order_planetscope_imagery_batch(dir = dir_data, df_coordinates = df_coordinates, v_site = c("HARV", "SJER"), v_year = 2025, setting = setting)
```

After successful ordering, check your [Planet account](https://www.planet.com/account/) to make sure orders are completed. Make sure all orders are "success" without any order "failed." Once all orders are completed, you might proceed to the next step of downloading. Please do not order the same images repeatedly.

> Why would you see failed orders?
> - You have exceeded your Planet account limit. If you have exceeded your limit, you can either wait for the limit to reset or contact Planet support to increase your limit.
> - The order is too large. You can try to order images for a smaller area or a shorter time period.
> - There were errors in the query, such as invalid coordinates. You can preview the images in your Planet Account.
> - Sometimes an order may fail due to temporary issues with the Planet API. In this case, you can try reordering the images after a few hours or days. Instead of reordering the entire batch, you can reorder only the failed sites and years.

### 2.3 Download

Use the `download_planetscope_imagery_batch()` function to download the ordered images. This function will save downloaded images to the folder `raw/` in the data directory. If you do not specify `v_site` and `v_year`, the function will download all ordered images in the `raw/` folder. This step uses multiple cores to download. Use `num_cores = 12` to download data from 12 months in parallel. `overwrite = F` prevents overwriting existing images.

If you would like to customize your batch downloading process, you may use the function `download_planetscope_imagery()` to download images from a single order. You can find the order ID saved in the `raw/` folder, or you can find it in the past orders in your Planet account, if you did not use `order_planetscope_imagery_batch()` to order images.

```{r, eval = F}
download_planetscope_imagery_batch(dir = dir_data, v_site = c("HARV", "SJER"), v_year = 2025, setting = setting, num_cores = 3, overwrite = F)
```

At this point, you might have downloaded a large amount of images. Consider archiving these raw images and removing the local copy after you have completed your downstream analyses.

### 2.4 Visualize true color imagery

You can visualize the true color imagery of single images using the `visualize_true_color_imagery()` function. You can overlay the coordinates from the relevant site on the imagery (optional). You can specify the brightness of the image using the `brightness` parameter.
```{r}
df_coordinates_SJER <- df_coordinates %>% filter(site == "SJER")

visualize_true_color_imagery(
  file = system.file("extdata", "raw/SJER/SJER_2025_60_90/20250318_190514_92_24d3_3B_AnalyticMS_SR_harmonized_clip.tif", package = "batchplanet"),
  df_coordinates = df_coordinates_SJER,
  brightness = 5
)
```

The `visualize_true_color_imagery_batch()` function starts a shiny app to visualize multiple images in the data directory. In the app, you can then choose the site and time you want to visualize and toggle brightness. Again, you have the option to overlay the coordinates on the images.
```{r, eval = F}
visualize_true_color_imagery_batch(dir = dir_data, df_coordinates = df_coordinates)
```

## 3 Retrieve and process time series

### 3.1 Retrieve time series

Extract time series at tree locations. If you are using HPC, use multiple cores for this step. Each core reads a map layer in parallel.
```{r, eval = F}
retrieve_planetscope_time_series_batch(dir = dir_data, df_coordinates = df_coordinates, v_site = c("HARV", "SJER"), v_group = c("Acer", "Quercus"), max_sample = 2000, num_cores = 10)
```

Read in data and visualize.
```{r}
df_ts <- read_data_product(dir = dir_data, v_site = c("HARV", "SJER"), v_group = c("Acer", "Quercus"), product_type = "ts")
visualize_time_series(df_ts, var = "green", ylab = "Green reflectance", facet_var = "site", smooth = F)
```

### 3.2 Time series preprocessing
Removing low quality data and calculating EVI.
```{r, eval = F}
clean_planetscope_time_series_batch(dir = dir_data, v_site = c("HARV", "SJER"), v_group = c("Acer", "Quercus"), num_cores = 3, calculate_evi = T)
```

Read in data and visualize.
```{r}
df_clean <- read_data_product(dir = dir_data, v_site = c("HARV", "SJER"), v_group = c("Quercus"), product_type = "clean")
visualize_time_series(df_clean, var = "evi", ylab = "EVI", facet_var = "site", smooth = T)
```

### 3.3 Calculate and Visualize Phenological Metrics

Now let's calculate day of year metrics.

```{r}
df_thres <- set_thresholds(thres_up = c(0.3, 0.4, 0.5), thres_down = NULL)

calculate_phenological_metrics_batch(dir = dir_data, v_site = "SJER", v_group = "Quercus", df_thres = df_thres, min_days = 20, check_seasonality = F, var_index = "evi", extend_to_previous_year = 275, extend_to_next_year = 90, num_cores = 1)
```

```{r}
v_id <- c("NEON.PLA.D17.SJER.06001", "NEON.PLA.D17.SJER.06337", "NEON.PLA.D17.SJER.06310")
# Read and visualize the DOY results
df_doy_sample <- read_data_product(dir = dir_data, v_site = "SJER", v_group = "Quercus", product_type = "doy") %>%
  filter(id %in% v_id)

df_evi_sample <- read_data_product(dir = dir_data, v_site = "SJER", v_group = "Quercus", product_type = "clean") %>%
  filter(id %in% v_id)

visualize_time_series(df_ts = df_evi_sample, df_doy = df_doy_sample, var = "evi", ylab = "EVI", facet_var = "id", smooth = T)
```

## 4 Misc Time Series Processing Tools

```{r}
t <- 1:365

# Double logistic function: leaf-on and leaf-off phases
double_logistic <- function(t, L = 0.1, U = 0.6, k1 = 0.1, k2 = 0.1, t1 = 120, t2 = 280) {
  L + (U - L) * (1 / (1 + exp(-k1 * (t - t1)))) * (1 / (1 + exp(k2 * (t - t2))))
}

# Generate simulate_ts values using double logistic
simulate_ts <- double_logistic(t)

# Add Gaussian noise
set.seed(42)
simulate_ts <- simulate_ts + rnorm(length(t), mean = 0, sd = 0.1)

# Introduce missing data (e.g., cloud cover)
missing_idx <- sample(1:length(t), size = round(0.2 * length(t))) # 10% missing
simulate_ts[missing_idx] <- NA

smoothed_ts <- whittaker_smoothing_filling(
  x = simulate_ts,
  maxgap = 30,
  lambda = 50,
  minseg = 2
)

# Compare original and smoothed time series
df_compare <- data.frame(
  original = simulate_ts,
  smoothed = smoothed_ts
) %>%
  mutate(timestep = row_number())

ggplot(df_compare) +
  geom_point(aes(x = timestep, y = original, color = "original")) +
  geom_line(aes(x = timestep, y = smoothed, color = "smoothed"), linewidth = 2, alpha = 0.75) +
  scale_color_manual(values = c("original" = "black", "smoothed" = "dark green")) +
  theme_minimal() +
  labs(
    x = "Timestep",
    y = "Value",
    color = ""
  ) +
  theme(legend.position = "bottom")
```

```{r}
# Check if a time series is flat
example_seasonality <- determine_seasonality(
  ts = simulate_ts,
  k = 50
)

print(str_c("Time series has significant seasonal variation: ", example_flat_check))
```
